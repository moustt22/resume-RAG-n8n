# Architecture of Resume RAG Chatbot

This document explains the end-to-end architecture of the multi-user Resume Chatbot system.

## 1. Frontend (Lovable)
- Users interact through a chat interface.
- Resume PDFs are uploaded via the frontend.
- Each user session is assigned a unique `session_id` (UUID).
- All requests (upload or question) include `session_id`.

## 2. Backend (n8n)
- Receives requests from Lovable webhook.
- If a PDF is uploaded:
  - Extract text from PDF
  - Chunk text (300–500 tokens, 50–80 overlap)
  - Generate embeddings (OpenAI-compatible, 1536 dims)
  - Upsert chunks to Pinecone in **namespace = resume-{session_id}**
- If a question is sent:
  - Query Pinecone using **namespace = resume-{session_id}**
  - Retrieve top K relevant chunks
  - Send chunks to LLM with system prompt:
    "Answer only from the provided context. If the answer is not in the document, say 'Not mentioned.'"

## 3. Vector Database (Pinecone)
- Each session gets its own namespace (`resume-{session_id}`)
- Prevents multi-user document mixing
- Allows safe parallel usage

## 4. LLM / Embeddings
- Embeddings: OpenAI-compatible (1536 dimensions)
- LLM: OpenRouter / OpenAI
- Embeddings used for semantic search
- LLM uses retrieved context to generate precise answers

## 5. Workflow Summary
1. User uploads resume → session_id created → PDF processed → embeddings stored in Pinecone namespace
2. User asks question → session_id sent → namespace queried → LLM generates answer → response returned to frontend

## 6. Security and Isolation
- session_id ensures per-user isolation
- No cross-session data leakage
- No user data stored outside session-specific namespace
